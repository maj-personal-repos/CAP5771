{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Exploratory Data Analysis\n",
    "## CAP 5771 - Principles of Data Mining\n",
    "### Professor: Miguel Alonso Jr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Please complete this lab assignment. Follow along carefully and answer all questions. Coding tasks should be coded inline within the jupyter notebook. No additional files are need or accepted unless otherwise noted. All data files can be found in the data folder on the github repo. Please read the contents of each section, as well as the comments in each cell. Add your code where it says 'Insert your code here'. Also, please be sure to answer any questions inline in markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "6000 \"best books\" were downloaded and assembled into a csv file. check out the  [Goodreads](https://goodreads.com). The \"rating\" of each book came is calculated by Goodreads and posted on their website. The csv file contains data for each book, including the book rating. The objective of this this lab is to clean and further parse the data. Then, you will do some exploratory data analysis to answer questions about these best books and popular genres.\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "\n",
    "1. Load a dataset and address missing values\n",
    "2. Parse columns in the dataframe to create new dataframe columns\n",
    "3. Create and interpret visualizations to explore the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploratory Data Analysis (EDA) Workflow\n",
    "\n",
    "The basic EDA workflow is as follows:\n",
    "\n",
    "1. **Build** a DataFrame from the data\n",
    "2. **Clean** the DataFrame with the following properites:\n",
    "    - Each row describes a single object\n",
    "    - Each column describes a property or feature of that object\n",
    "    - Columns are numeric whenever appropriate (easier to work with)\n",
    "    - Object properties are atomic that cannot be further decomposed\n",
    "3. Explore global properties using histograms, scatter plots, and aggregation functions to summarize the data.\n",
    "4. Explore group properties using groupby and small multiples to compare subsets of the data.\n",
    "\n",
    "This process transforms the data into a format which is easier to work with, gives a basic overview of the data's properties, and likely generates several questions for you to followup in subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "In order to process the data, you must load the data from the csv file, [goodreads.csv](). If you've cloned this repository, the data is located in a folder called data, located in the Labs folder of the repository. Otherwise, make sure that you download the file and load it with the appropriate path.\n",
    "\n",
    "Your task for this part is to load the data into a pandas dataframe and resolve any issues related to the loading of the data. Report any issues encountered during loading.\n",
    "\n",
    "You'll need the following libraries imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# some tweaks to the default options in pandas\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data into a dataframe\n",
    "\n",
    "The first step is to load the data into the dataframe from the csv file. Go ahead and try loading it. If you load the data as is, do you notice any issues about the data? You can try exploring the raw data file in another program such as excel or your favorite text editor to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data into a dataframe\n",
    "df = pd.read_csv(\"./data/goodreads.csv\")\n",
    "\n",
    "# Print out the first couple of rows of the dataframe\n",
    "####### \n",
    "#   Insert your code\n",
    "#######\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and read the csv file, adding the appropriate column headings described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/goodreads.csv\", header=None, names=['rating',\n",
    "                                                           'review_count',\n",
    "                                                           'isbn',\n",
    "                                                           'booktype',\n",
    "                                                           'author_url',\n",
    "                                                           'year',\n",
    "                                                           'genre_urls',\n",
    "                                                           'dir',\n",
    "                                                           'rating_count',\n",
    "                                                           'name'])\n",
    "\n",
    "# Print out the first couple of rows of the dataframe\n",
    "####### \n",
    "#   Insert your code\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick checks\n",
    "\n",
    "It's always a good idea to examine the data before doing any further preprocessing or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column data types\n",
    "####### \n",
    "#   Insert your code\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else should be checked? Research other important checks to make on the dataset before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few more properties of the dataset before moving on\n",
    "####### \n",
    "#   Insert your code\n",
    "####### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Examining a bit further\n",
    "\n",
    "Many times, we need to examine the data further before moving on. One thing that's always important to check is to make sure that the data does not have any missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the data read correctly? Were the values represented properly, as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining a bit further\n",
    "\n",
    "Many times, we need to examine the data further before moving on. Sure checking the properties of the dataset are important, as is checking the first $n$ rows. But one thing that's always important to check is to make sure that the data does not have any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see how many missing values there are in the dataframe\n",
    "####### \n",
    "#   Insert your code\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, locate where the missing values occur\n",
    "####### \n",
    "#   Insert your code\n",
    "####### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do libraries like [pandas](https://pandas.pydata.org/) or [numpy](https://numpy.org/) handle missing values when trying to compute using datasets that contain them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating missing values\n",
    "\n",
    "So something needs to be done about 'missing' or 'invalid' values in the data (hint: check where they occur)? One thing that can be done is exclude them from the dataframe altogther. That begs the question: Is this appropriate for all 'missing'/'invalid' values? How would you drop these values from the dataframe (hint: is it possible to eliminate just a single entry in your dataframe? Should you eliminate an entire row? Or how about the entire column?)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address the missing or invalid values in your dataframe\n",
    "####### \n",
    "#   Insert your code\n",
    "####### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always check your work. Is it enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column data types again\n",
    "####### \n",
    "#   Insert your code\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The float has not yet changed. Those types need to be fixed with a type conversion. If this fails, then further exploration to correct the issue is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rating_count, review_count and year to int \n",
    "#######\n",
    "# .Insert your code\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other columns that have NaN should also be handled, such as string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.genre_urls.isnull(), 'genre_urls']=\"\"\n",
    "df.loc[df.isbn.isnull(), 'isbn']=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking questions\n",
    "\n",
    "Recall the goal of data mining is to extract knowledge from data. So, there should always be questions to ask. Think of few questions and then examine the data and decide if the dataframe contains what you need to address these questions.\n",
    "\n",
    "Example: Which are the highest rated books? To determine this, you'll only need the data in two columns: name and rating. The task will be to sort these two columns by the value in rating. [5 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and Completing the data\n",
    "\n",
    "Sometimes, columns contain the information that we need, but not in the right form. Typically, we will need to do additional processing on these columns to create new columns with the right information. Take a look at the author_url or genre_url columns. There's information in those entries that we'd like to keep (like the author names and genres) but, there's additional information that's not that helpful. Pandas has features, such as the map feature, that allow us to assign new columns to the dataframe, along with some type of transformation. We'll be using that feature to add two new columns: **author** and **genre**.\n",
    "\n",
    "---\n",
    "\n",
    "First things first: explore the author_url and genre_url columns to understand how the urls are structured and then formulate what type of string operations are required to isolate the author's name and the genres the books belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first author_url\n",
    "author_test_string = df.author_url[0]\n",
    "author_test_string\n",
    "\n",
    "# Apply some string operations to isolate the author name\n",
    "####### \n",
    "#   Insert your code\n",
    "####### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine some examples of genre_urls\n",
    "# Insert your code here\n",
    "\n",
    "# Apply some string operations to isolate the genre name\n",
    "####### \n",
    "#   Insert your code\n",
    "####### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and write two functions, one that takes an author_url and returns the author's name, and the second, a function that takes the genre_urls, and returns the genre names, separated by a pipe character \"|\" (known as the delimeter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that accepts an author url and returns the author's name\n",
    "def extract_author(url):\n",
    "    ####### \n",
    "    #   Insert your code\n",
    "    ####### \n",
    "    return name\n",
    "\n",
    "#Apply the get_author function to the 'author_url' column using '.map' \n",
    "#and add a new column 'author' to store the names\n",
    "df['author'] = df.author_url.map(extract_author)\n",
    "df.author[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function that accepts a genre url and returns the genres as a string, separated by a pipe character\n",
    "def extract_genres(url):\n",
    "    ####### \n",
    "    #   Insert your code\n",
    "    ####### \n",
    "    return genre\n",
    "\n",
    "df['genres']=df.genre_urls.map(extract_genres)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "The next step, after some basic data preparation and preprocessing, is to create some basic visualizations, such as histograms (with both linear and log-linear scales), from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram of some of the columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms using the format df.YOUR_CHOICE_OF_COLUMN_NAME.hist(bins=YOUR_CHOICE_OF_BIN_SIZE)\n",
    "####### \n",
    "#   Insert your code\n",
    "####### \n",
    "\n",
    "plt.xlabel('Label the x-axis appropriately')\n",
    "plt.ylabel('Label the y-axis appropriately')\n",
    "plt.title('Title the plot appropriately')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the parameters chosen don't produce useful graphs. Go ahead and vary those parameters until the histograms make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate histograms using the format df.YOUR_CHOICE_OF_COLUMN_NAME.hist(bins=YOUR_CHOICE_OF_BIN_SIZE)\n",
    "# Be sure to vary the parameters, e.g. bin size, in order to produce useful graphs\n",
    "####### \n",
    "#   Insert your code\n",
    "####### \n",
    "\n",
    "plt.xlabel('Label the x-axis appropriately')\n",
    "plt.ylabel('Label the y-axis appropriately')\n",
    "plt.title('Title the plot appropriately')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best books\n",
    "\n",
    "Pandas is a great framework for analyzing datasets. It includes many useful features that allows for taking different view on the data. For example, the groupby function of a dataframe allows for grouping the data by one of the features (columns). This is an example of analyzing the dataset by a \"grouped property\" type.\n",
    "\n",
    "For the goodreads dataset, use the groupby feature to find the \"best book\" by year. Think about what it means to be the \"best book\" and use it to determine the best book by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .groupby, we can divide the dataframe into subsets by the values of 'year'.\n",
    "# The groupby function returns an iterator that can be used to iterate through the\n",
    "# dataset and calculate over the subsets. Save the results into a new dataframe with\n",
    "# one column for year and one column for book of the year and print the first few records\n",
    "# of the dataset.\n",
    "for year, subset in df.groupby('year'):\n",
    "    # Find the best book of the year\n",
    "    ####### \n",
    "    #   Insert your code\n",
    "    #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this for other definitions of \"best book\" using the groupby dataframe function, e.g. find the best book by author, define best book by review count, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends in the data\n",
    "\n",
    "Thinking about the previous example of using groupby, we can take a look at trends in the data. For example, we can try to answer the following questions:\n",
    "1. Which is the most popular genre in any give month?\n",
    "2. What's the overall most popular genre?\n",
    "3. What conclusions can be drawn about the popularity of genres over time?\n",
    "\n",
    "To answer these questions, we first need to find the unique genres in the dataset. Recall that each genre string is a pipe (|) separated list of genres. We'll start by identifying if the genre string is indeed a pipe separated list. If it is, we return True, otherwise we return False. Then, we can parse those pip separated genre strings into the individual genres and add them to a python set (python sets only contain unique elements, regardless of how many times an element is added to the set). We can return the set once we've iterated through the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the unique genres contained in the dataframe.\n",
    "####### \n",
    "#   Insert your code\n",
    "####### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we add a column to the dataframe for each genre? Is this way of representing genre efficient? Does it allows for easy computation and visualization? Are there alternative ways to represent the genre information that allow for easy visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to the dataframe for each genre\n",
    "####### \n",
    "#   Insert your code\n",
    "####### \n",
    "\n",
    "# check the size of the new dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and create some visualizations of the genres in the dataset, e.g., create a visualization that allows for us to identify which is the most represented genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Pick three genres and describe how the popularity of these genres fluctuates with time. Go ahead and produce a time series representation of the genre popularity to aid in your description."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
